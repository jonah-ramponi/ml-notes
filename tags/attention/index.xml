<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Attention on Jonah&#39;s ML Notes</title>
    <link>https://www.jonahramponi.com/tags/attention/</link>
    <description>Recent content in Attention on Jonah&#39;s ML Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 30 Mar 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://www.jonahramponi.com/tags/attention/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Intro to Attention</title>
      <link>https://www.jonahramponi.com/posts/intro_to_attention/</link>
      <pubDate>Sat, 30 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://www.jonahramponi.com/posts/intro_to_attention/</guid>
      <description>Suppose you give an LLM the input&#xA;What is the capital of France?&#xA;The first thing the LLM will do is split this input into tokens. A token is just some combinations of characters. You can see an example of the tokenization outputs for the question below.&#xA;$\colorbox{red}{What}\colorbox{magenta}{ is}\colorbox{green}{ the}\colorbox{orange}{ capital}\colorbox{purple}{ of}\colorbox{brown}{ France}\colorbox{cyan}?$&#xA;(This tokenization was produced using cl100k_base, the tokenizer used in GPT-3.5-turbo and GPT-4.)&#xA;In this example we have $(n = 7)$ tokens.</description>
    </item>
    <item>
      <title>Fancy Pants Attention Techniques</title>
      <link>https://www.jonahramponi.com/posts/fancy_pants_attention/</link>
      <pubDate>Sat, 23 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://www.jonahramponi.com/posts/fancy_pants_attention/</guid>
      <description>Sparse Attention Sparse Attention introduces sparse factorizations on the attention matrix. To implement this we introduce a connectivity pattern $S = {S_1,\dots,S_n}$. Here, $S_i$ denotes the set of indices of the input vectors to which the $i$th output vector attends. For instance, in regular $n^2$ attention every input vector attends to every output vector before it in the sequence. Remember that $d_k$ is the inner dimension of our queries and keys.</description>
    </item>
  </channel>
</rss>
