<!DOCTYPE html>
<html><head lang="en">
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Flash Attention - Jonah&#39;s ML Notes</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="Reduce the memory usage used to compute exact attention." />
	<meta property="og:image" content=""/>
	<meta property="og:title" content="Flash Attention" />
<meta property="og:description" content="Reduce the memory usage used to compute exact attention." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.jonahramponi.com/posts/flash_attention/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-03-26T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-03-26T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Flash Attention"/>
<meta name="twitter:description" content="Reduce the memory usage used to compute exact attention."/>
<script src="https://www.jonahramponi.com/js/feather.min.js"></script>
	
	
        <link href="https://www.jonahramponi.com/css/fonts.11a1877508139eac0b5b4852ceb110c35641b3533321e66e39149e901ed5756b.css" rel="stylesheet">
	

	
	<link rel="stylesheet" type="text/css" media="screen" href="https://www.jonahramponi.com/css/main.d902908ac6e0fab67957de5db5aea1b6455b19ae2ca98eac4c95a4a0fdc02238.css" />
		<link id="darkModeStyle" rel="stylesheet" type="text/css" href="https://www.jonahramponi.com/css/dark.c95c5dcf5f32f8b67bd36f7dab66680e068fce2b303087294114aabf7a7c080b.css"  disabled />
	

	
	
		<script type="text/javascript"
		src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>
	
		
		<script type="text/x-mathjax-config">
		MathJax.Hub.Config({
			tex2jax: {
				inlineMath: [['$','$'], ['\\(','\\)']],
				displayMath: [['$$','$$'], ['\[','\]']],
				processEscapes: true,
				processEnvironments: true,
				skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
				TeX: { equationNumbers: { autoNumber: "AMS" },
						 extensions: ["AMSmath.js", "AMSsymbols.js"] }
			}
		});
		</script>
	

	
	
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css">
		<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"></script>
		<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
		
		
		<script>
			document.addEventListener("DOMContentLoaded", function() {
					renderMathInElement(document.body, {
							delimiters: [
									{left: "$$", right: "$$", display: true},
									{left: "$", right: "$", display: false}
							]
					});
			});
			</script>
	
	
	
</head>
<body>
        <div class="content"><header>
	<div class="main">
		<a href="https://www.jonahramponi.com/">Jonah&#39;s ML Notes</a>
	</div>
	<nav>
		
		<a href="/">Home</a>
		
		<a href="/about">About</a>
		
		| <span id="dark-mode-toggle" onclick="toggleTheme()"></span>
		<script src="https://www.jonahramponi.com/js/themetoggle.js"></script>
		
	</nav>
</header>

<main>
	<article>
		<div class="title">
			<h1 class="title">Flash Attention</h1>
			<div class="meta">Posted on Mar 26, 2024</div>
		</div>
		
		<div class="tldr">
			<strong>tl;dr:</strong>
			Reduce the memory usage used to compute exact attention.
		</div>

		<section class="body">
			<p>The goal of <a href="https://arxiv.org/pdf/2205.14135.pdf"><em>Flash Attention</em></a> is to compute the attention value with fewer high bandwidth memory read / writes. The approach has since been refined in  <a href="https://arxiv.org/pdf/2307.08691.pdf"><em>Flash Attention 2</em></a>.</p>
<p>We will split the attention inputs $Q,K,V$ into blocks. Each block will be handled separately, and attention will therefore be computed with respect to each block. With the correct scaling, adding the outputs from each block we will give us the same attention value as we would get by computing everything all together.</p>
<p><strong>Tilling.</strong> To compute attention, we multiply $Q \times K^T$, divide by $\sqrt{d_k}$ and then take the softmax. Keeping track of the scaling values in softmax is the key to making this technique work. The softmax for a vector $\vec{x} \in \mathbb{R}^{2n}$ is given by</p>
<p>$$
m(x):= \max_i x_i, \hspace{3mm} f(x):= [e^{x_1-m(x)}, \dots, e^{x_b -m(x)}], \hspace{3mm} \ell(x) := \sum_i f(x)_i, \hspace{3mm} \text{softmax}(x) := \frac{f(x)}{\ell(x)}.
$$</p>
<p>This looks unfriendly, but is really just the notation for a more numerically stable softmax. What does that mean? Well, notice we are just applying regular softmax but with some shifting of each element of vector $\vec{x}$ by $\max(x)$ units. We can do this because softmax$(\vec{x}) = \text{softmax}(\vec{x}-c)$ for any scalar $c$.</p>
<p><em>Proof</em>
\begin{align*}
\text{softmax}(\vec{x} - c) &amp;= \frac{e^{\vec{x} - c}}{\sum_{j} e^{x_j - c}} \\
&amp;= \frac{e^{\vec{x}} \cdot e^{-c}}{\sum_{j} e^{x_j} \cdot e^{-c}} \\
&amp;= \frac{e^{\vec{x}}}{\sum_{j} e^{x_j}} \\
&amp;= \text{softmax}(\vec{x})
\end{align*}</p>
<p>In this case, we improve numerical stability by ensuring we do not take the exponential of very large numbers. This can lead to overflow issues. This simply means our number gets too big to store in the given datatype. By subtracting the largest element, we ensure the vector $\vec{x}$ only has non-positive entries. For example, in floating point 64, the maximum value we can represent is very large $(10^{308})$. However</p>
<p>$$
e^x &gt; 10^{308} \implies x &gt; \ln(10^{308}) \implies x &gt; 308 \times \ln(10) \implies x &gt; 709.
$$</p>
<p>Therefore, approximately any $x$ larger than $709$ will result in overflow issues. For instance, computing $\exp(709) = 8.22e+307$ but $\exp(710) = inf$ in <em>numpy</em>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>np<span style="color:#f92672">.</span>exp(<span style="color:#ae81ff">709</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 8.218407461554972e+307</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>np<span style="color:#f92672">.</span>exp(<span style="color:#ae81ff">710</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># &lt;stdin&gt;:1: RuntimeWarning: overflow encountered in exp</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># inf</span>
</span></span></code></pre></div><p>We certainly do not want our model to hit any overflow errors. It is therefore preferable to use this numerically stable version of softmax.</p>
<p>To compute softmax in blocks, we decompose our vector $\vec{x} \in \mathbb{R}^{2n}$ into two smaller vectors in $\mathbb{R}^n$.Let&rsquo;s look at the simple case of decomposing into two vectors. Denote these vectors $\vec{x}_1,\vec{x}_2$ each in $\mathbb{R}^n$. Our softmax calculation becomes</p>
<p>\begin{aligned}
m(x) &amp;= m([x_1\hspace{3mm}  x_2]) = \max (m(x_1),m(x_2)), \\
f(x) &amp;= [e^{m(x_1) - m(x)}f(x_1) \hspace{3mm} e^{m(x_2) - m(x)}f(x_2)], \\
\ell(x) &amp;= \ell([x_1\hspace{3mm}  x_2]) = [e^{m(x_1) - m(x)}\ell(x_1) \hspace{3mm} e^{m(x_2) - m(x)}\ell(x_2)], \\
\text{softmax}(x) &amp;= \frac{f(x)}{\ell(x)}.
\end{aligned}</p>
<p>Notice that we use $m(x_i) - m(x)$ as the normalization factor, as we do not know which group will contain the maximum value of $\vec{x}$. By keeping track of both $m(x)$ and $\ell(x)$ we will be able to accurately recombine the softmax outputs for each block, as will know how to rescale the softmax outputs.</p>
<p><strong>Recomputation.</strong> We also do not wish to store all the intermediate values we calculate for every backward pass. Typically we require the attention matrix, $QK^T$, and the output after softmax, simply softmax($QK^T$) in each backward pass. However, by using our blocks of $Q,K,V$ the whole attention matrix is not required to be loaded in during every backward pass.</p>

		</section>

		<div class="post-tags">
			
			
			<nav class="nav tags">
				<ul class="tags">
					
					<li><a href="/tags/attention">attention</a></li>
					
					<li><a href="/tags/inference">inference</a></li>
					
				</ul>
			</nav>
			
			
		</div>
		</article>
</main>
<footer>
  <div style="display:flex"></div>
  <div class="footer-info">
    2024  <a
      href="https://github.com/athul/archie">Archie Theme</a> | Built with <a href="https://gohugo.io">Hugo</a>
  </div>
</footer>


</div>
    </body>
</html>
