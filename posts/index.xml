<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Jonah&#39;s ML Notes</title>
    <link>https://www.jonahramponi.com/posts/</link>
    <description>Recent content in Posts on Jonah&#39;s ML Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 Aug 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://www.jonahramponi.com/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Why did Meta publish the Llama models for free?</title>
      <link>https://www.jonahramponi.com/posts/meta_analysis/</link>
      <pubDate>Fri, 23 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://www.jonahramponi.com/posts/meta_analysis/</guid>
      <description>To understand why Meta has open sourced the Llama family of models, it is important to understand how Meta makes money. Meta makes money from adverts. Almost their entire revenue comes from adverts (1). So,&#xA;why have Meta invested so much money into the Llama models?&#xA;Probably, to make more money from adverts.&#xA;Here are some ways in which the open source release of the Llama models might help Meta make more money from adverts.</description>
    </item>
    <item>
      <title>Intro to Attention</title>
      <link>https://www.jonahramponi.com/posts/intro_to_attention/</link>
      <pubDate>Sat, 30 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://www.jonahramponi.com/posts/intro_to_attention/</guid>
      <description>Suppose you give an LLM the input&#xA;What is the capital of France?&#xA;The first thing the LLM will do is split this input into tokens. A token is just some combinations of characters. You can see an example of the tokenization outputs for the question below.&#xA;$\colorbox{red}{What}\colorbox{magenta}{ is}\colorbox{green}{ the}\colorbox{orange}{ capital}\colorbox{purple}{ of}\colorbox{brown}{ France}\colorbox{cyan}?$&#xA;(This tokenization was produced using cl100k_base, the tokenizer used in GPT-3.5-turbo and GPT-4.)&#xA;In this example we have $(n = 7)$ tokens.</description>
    </item>
    <item>
      <title>LoRa</title>
      <link>https://www.jonahramponi.com/posts/finetuning/</link>
      <pubDate>Sun, 24 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://www.jonahramponi.com/posts/finetuning/</guid>
      <description>Let&amp;rsquo;s consider a weight matrix $W$. Typically, the weight matrices in a dense neural networks layers have full-rank. Full-rank means many different things mathematically. I think the easiest explanation of a $d$-dimensional matrix (let&amp;rsquo;s consider a square matrix $,M \in \mathbb{R}^{d,d}$) being full-rank is one in which the columns could be used to span (hit every point) in $d$-dimensional space. If you consider $d=3$, a matrix like&#xA;\begin{equation} M = \begin{pmatrix} 1 &amp;amp; 0 &amp;amp; 1 \\ 1 &amp;amp; 0 &amp;amp; 1 \\ 1 &amp;amp; 1 &amp;amp; 0 \end{pmatrix} \end{equation}</description>
    </item>
    <item>
      <title>Fancy Pants Attention Techniques</title>
      <link>https://www.jonahramponi.com/posts/fancy_pants_attention/</link>
      <pubDate>Sat, 23 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://www.jonahramponi.com/posts/fancy_pants_attention/</guid>
      <description>Sparse Attention Sparse Attention introduces sparse factorizations on the attention matrix. To implement this we introduce a connectivity pattern $S = {S_1,\dots,S_n}$. Here, $S_i$ denotes the set of indices of the input vectors to which the $i$th output vector attends. For instance, in regular $n^2$ attention every input vector attends to every output vector before it in the sequence. Remember that $d_k$ is the inner dimension of our queries and keys.</description>
    </item>
  </channel>
</rss>
