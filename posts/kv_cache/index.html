<!DOCTYPE html>
<html><head lang="en">
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>The KV Cache - Jonah&#39;s ML Notes</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="Computing the attention more efficiently at inference." />
	<meta property="og:image" content=""/>
	<meta property="og:title" content="The KV Cache" />
<meta property="og:description" content="Computing the attention more efficiently at inference." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.jonahramponi.com/posts/kv_cache/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-03-22T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-03-22T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="The KV Cache"/>
<meta name="twitter:description" content="Computing the attention more efficiently at inference."/>
<script src="https://www.jonahramponi.com/js/feather.min.js"></script>
	
	
        <link href="https://www.jonahramponi.com/css/fonts.11a1877508139eac0b5b4852ceb110c35641b3533321e66e39149e901ed5756b.css" rel="stylesheet">
	

	
	<link rel="stylesheet" type="text/css" media="screen" href="https://www.jonahramponi.com/css/main.d902908ac6e0fab67957de5db5aea1b6455b19ae2ca98eac4c95a4a0fdc02238.css" />
		<link id="darkModeStyle" rel="stylesheet" type="text/css" href="https://www.jonahramponi.com/css/dark.c95c5dcf5f32f8b67bd36f7dab66680e068fce2b303087294114aabf7a7c080b.css"  disabled />
	

	
	
		<script type="text/javascript"
		src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>
	
		
		<script type="text/x-mathjax-config">
		MathJax.Hub.Config({
			tex2jax: {
				inlineMath: [['$','$'], ['\\(','\\)']],
				displayMath: [['$$','$$'], ['\[','\]']],
				processEscapes: true,
				processEnvironments: true,
				skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
				TeX: { equationNumbers: { autoNumber: "AMS" },
						 extensions: ["AMSmath.js", "AMSsymbols.js"] }
			}
		});
		</script>
	

	
	
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css">
		<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"></script>
		<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
		
		
		<script>
			document.addEventListener("DOMContentLoaded", function() {
					renderMathInElement(document.body, {
							delimiters: [
									{left: "$$", right: "$$", display: true},
									{left: "$", right: "$", display: false}
							]
					});
			});
			</script>
	
	
	
</head>
<body>
        <div class="content"><header>
	<div class="main">
		<a href="https://www.jonahramponi.com/">Jonah&#39;s ML Notes</a>
	</div>
	<nav>
		
		<a href="/">Home</a>
		
		<a href="/about">About</a>
		
		| <span id="dark-mode-toggle" onclick="toggleTheme()"></span>
		<script src="https://www.jonahramponi.com/js/themetoggle.js"></script>
		
	</nav>
</header>

<main>
	<article>
		<div class="title">
			<h1 class="title">The KV Cache</h1>
			<div class="meta">Posted on Mar 22, 2024</div>
		</div>
		
		<div class="tldr">
			<strong>tl;dr:</strong>
			Computing the attention more efficiently at inference.
		</div>

		<section class="body">
			<p>The computation of attention is costly. Remember that our decoder works in an auto-regressive fashion. For our given input <em>$$\colorbox{red}{What}\colorbox{magenta}{ is}\colorbox{green}{ the}\colorbox{orange}{ capital}\colorbox{purple}{ of}\colorbox{brown}{ France}\colorbox{cyan}{?}&quot;$$</em></p>
<p>\begin{align}
\text{Prediction 1} &amp;= \colorbox{orange}{The} \\
\text{Prediction 2} &amp;= \colorbox{orange}{The}\colorbox{pink}{ capital} \\
&amp;\vdots \\
\text{Prediction $p$} &amp;= \colorbox{orange}{The}\colorbox{pink}{ capital} (\dots) \colorbox{red}{ Paris.}
\end{align}</p>
<p>To produce prediction $2$, we will take the output from prediction $1$. At each step, the model will also see our input sequence. Without any tricks, at every step, we&rsquo;re going to be re-computing values that have already been calculated. Our attention matrix used for our first prediction will have the following structure</p>
<p><img src="/img/first_pred_kv.png" alt="Sliding Window Attention Visual"></p>
<p>When we compute the second prediction, the structure of our attention matrix looks very similar. Notice that the attention matrix after prediction one is actually contained within this matrix!</p>
<p><img src="/img/second_pred_kv.png" alt="Dilated Sliding Window Attention Visual"></p>
<p>Remember, $Q$ and $K^T$ are just defined by our matrix $M$ which contains one row per input token. Thus, $Q$ and $K^T$ are very similar between the first and second predictions - only one row / column has changed! By caching $K$ for each prediction, we can make the computation of our attention matrix more efficient and by caching $V$, we make our attention mechanism output calculation more efficient.</p>

		</section>

		<div class="post-tags">
			
			
			<nav class="nav tags">
				<ul class="tags">
					
					<li><a href="/tags/attention">attention</a></li>
					
					<li><a href="/tags/inference">inference</a></li>
					
				</ul>
			</nav>
			
			
		</div>
		</article>
</main>
<footer>
  <div style="display:flex"></div>
  <div class="footer-info">
    2024  <a
      href="https://github.com/athul/archie">Archie Theme</a> | Built with <a href="https://gohugo.io">Hugo</a>
  </div>
</footer>


</div>
    </body>
</html>
